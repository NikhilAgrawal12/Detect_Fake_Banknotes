{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83b4b8a-1d86-42fd-82cf-0e700d39fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6b22659-205f-42bf-97a3-2343ccfbe2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"data_banknote_authentication.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "197c76a7-43ac-40d7-adfd-49766a3fbdfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.4501</td>\n",
       "      <td>-0.55949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.4774</td>\n",
       "      <td>0.34179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.77710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.28230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.19520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis  entropy  class\n",
       "0      3.62160   8.66610   -2.8073 -0.44699      0\n",
       "1      4.54590   8.16740   -2.4586 -1.46210      0\n",
       "2      3.86600  -2.63830    1.9242  0.10645      0\n",
       "3      3.45660   9.52280   -4.0112 -3.59440      0\n",
       "4      0.32924  -4.45520    4.5718 -0.98880      0\n",
       "...        ...       ...       ...      ...    ...\n",
       "1367   0.40614   1.34920   -1.4501 -0.55949      1\n",
       "1368  -1.38870  -4.87730    6.4774  0.34179      1\n",
       "1369  -3.75030 -13.45860   17.5932 -2.77710      1\n",
       "1370  -3.56370  -8.38270   12.3930 -1.28230      1\n",
       "1371  -2.54190  -0.65804    2.6842  1.19520      1\n",
       "\n",
       "[1372 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1440e0-fe4c-4235-9bff-1aeeaf2e1fff",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d57ae18d-96af-450b-97e5-32d9151b3edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"color\"]=df[\"class\"].apply(lambda x:'green' if x==0 else 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef0912fc-c768-4c8a-a82b-3014264ebb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.4501</td>\n",
       "      <td>-0.55949</td>\n",
       "      <td>1</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.4774</td>\n",
       "      <td>0.34179</td>\n",
       "      <td>1</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.77710</td>\n",
       "      <td>1</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.28230</td>\n",
       "      <td>1</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.19520</td>\n",
       "      <td>1</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis  entropy  class  color\n",
       "0      3.62160   8.66610   -2.8073 -0.44699      0  green\n",
       "1      4.54590   8.16740   -2.4586 -1.46210      0  green\n",
       "2      3.86600  -2.63830    1.9242  0.10645      0  green\n",
       "3      3.45660   9.52280   -4.0112 -3.59440      0  green\n",
       "4      0.32924  -4.45520    4.5718 -0.98880      0  green\n",
       "...        ...       ...       ...      ...    ...    ...\n",
       "1367   0.40614   1.34920   -1.4501 -0.55949      1    red\n",
       "1368  -1.38870  -4.87730    6.4774  0.34179      1    red\n",
       "1369  -3.75030 -13.45860   17.5932 -2.77710      1    red\n",
       "1370  -3.56370  -8.38270   12.3930 -1.28230      1    red\n",
       "1371  -2.54190  -0.65804    2.6842  1.19520      1    red\n",
       "\n",
       "[1372 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def7ef7-f8e8-4a86-9eae-f274bb584000",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b165bf06-901c-437f-b11a-0a53b7ff0567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class  μ(f1)  σ(f1)  μ(f2)  σ(f2)  μ(f3)  σ(f3)  μ(f4)  σ(f4)\n",
      "    0   2.28   2.02   4.26   5.14   0.80   3.24  -1.15   2.13\n",
      "    1  -1.87   1.88  -0.99   5.40   2.15   5.26  -1.25   2.07\n",
      "  all   0.43   2.84   1.92   5.87   1.40   4.31  -1.19   2.10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the DataFrame is already loaded with the correct column names\n",
    "df.columns = ['variance', 'skewness', 'curtosis', 'entropy', 'class', 'color']\n",
    "\n",
    "# Exclude the 'color' column before aggregation\n",
    "numeric_df = df.drop(columns=['color'])\n",
    "\n",
    "# Calculate mean and standard deviation for each feature grouped by class\n",
    "stats_by_class = numeric_df.groupby('class').agg(['mean', 'std']).round(2)\n",
    "\n",
    "# Calculate overall mean and std for all rows\n",
    "overall_stats = numeric_df.agg(['mean', 'std']).round(2)\n",
    "\n",
    "# Reformat the DataFrame to match the given table structure\n",
    "formatted_table = pd.DataFrame({\n",
    "    'class': ['0', '1', 'all'],\n",
    "    'μ(f1)': [stats_by_class.loc[0, ('variance', 'mean')], stats_by_class.loc[1, ('variance', 'mean')], overall_stats.loc['mean', 'variance']],\n",
    "    'σ(f1)': [stats_by_class.loc[0, ('variance', 'std')], stats_by_class.loc[1, ('variance', 'std')], overall_stats.loc['std', 'variance']],\n",
    "    'μ(f2)': [stats_by_class.loc[0, ('skewness', 'mean')], stats_by_class.loc[1, ('skewness', 'mean')], overall_stats.loc['mean', 'skewness']],\n",
    "    'σ(f2)': [stats_by_class.loc[0, ('skewness', 'std')], stats_by_class.loc[1, ('skewness', 'std')], overall_stats.loc['std', 'skewness']],\n",
    "    'μ(f3)': [stats_by_class.loc[0, ('curtosis', 'mean')], stats_by_class.loc[1, ('curtosis', 'mean')], overall_stats.loc['mean', 'curtosis']],\n",
    "    'σ(f3)': [stats_by_class.loc[0, ('curtosis', 'std')], stats_by_class.loc[1, ('curtosis', 'std')], overall_stats.loc['std', 'curtosis']],\n",
    "    'μ(f4)': [stats_by_class.loc[0, ('entropy', 'mean')], stats_by_class.loc[1, ('entropy', 'mean')], overall_stats.loc['mean', 'entropy']],\n",
    "    'σ(f4)': [stats_by_class.loc[0, ('entropy', 'std')], stats_by_class.loc[1, ('entropy', 'std')], overall_stats.loc['std', 'entropy']],\n",
    "})\n",
    "\n",
    "# Display the formatted table\n",
    "print(formatted_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aa67e0-ac6f-46c4-abb1-9a48776798a7",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53bcc3a-724d-491c-9355-63922cf0555d",
   "metadata": {},
   "source": [
    "**Variance (f1):**\n",
    "\n",
    "The mean (μ(f1)) for class 0 (real banknotes) is positive (2.28), while for class 1 (fake banknotes) it is negative (-1.87). This indicates that real banknotes tend to have a higher variance in the wavelet transformed image compared to fake banknotes.\n",
    "\n",
    "The standard deviation (σ(f1)) is similar for both classes, indicating that the spread of variance values is not significantly different between real and fake banknotes.\n",
    "\n",
    "**Skewness (f2):**\n",
    "\n",
    "The mean skewness (μ(f2)) for class 0 is 4.26, much higher than that for class 1, which is -0.99. This suggests that real banknotes tend to exhibit more positive skewness compared to fake banknotes.\n",
    "\n",
    "The standard deviation (σ(f2)) is quite close for both classes, implying that the spread of skewness values is also not drastically different.\n",
    "\n",
    "**Curtosis (f3):**\n",
    "\n",
    "The mean curtosis (μ(f3)) for class 0 is 0.80, while for class 1 it is significantly higher at 2.15. This indicates that fake banknotes may exhibit higher peakedness in the distribution of wavelet transformed image values.\n",
    "\n",
    "The standard deviation (σ(f3)) is higher for class 1, suggesting more variability in curtosis among fake banknotes.\n",
    "\n",
    "**Entropy (f4):**\n",
    "\n",
    "Both classes have negative mean entropy values (μ(f4)), but class 1 has a slightly lower mean (-1.25) compared to class 0 (-1.15). This suggests that the entropy values for fake banknotes may be slightly more negative, indicating potentially more disorder in the image representation.\n",
    "\n",
    "The standard deviation (σ(f4)) is close for both classes, indicating similar variability in entropy values.\n",
    "\n",
    "**Overall Patterns:**\n",
    "\n",
    "Real banknotes generally have higher variance and skewness values, while fake banknotes exhibit higher curtosis.\n",
    "\n",
    "The differences in mean values across features suggest that variance, skewness, and curtosis might play significant roles in distinguishing between real and fake banknotes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902a681-d828-4ed5-a360-394317e2ce16",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b04b4bc9-e164-44f8-9fb1-f680fac17996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise plots have been saved as 'good_bills.pdf' and 'fake_bills.pdf'.\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into 50% training and 50% testing\n",
    "X_train, X_test = train_test_split(df, test_size=0.5, random_state=42, stratify=df['class'])\n",
    "\n",
    "# Define custom color palettes\n",
    "good_palette = {'green': 'green'}\n",
    "fake_palette = {'red': 'red'}\n",
    "\n",
    "# Plot pairwise relationships for class 0 (good bills) with green color\n",
    "good_bills = X_train[X_train['class'] == 0]\n",
    "sns.pairplot(good_bills, hue='color', diag_kind='hist', palette=good_palette)\n",
    "plt.suptitle('Pairwise Relationships for Good Bills', y=1.02)\n",
    "plt.savefig('good_bills.pdf')\n",
    "plt.close()\n",
    "\n",
    "# Plot pairwise relationships for class 1 (fake bills) with red color\n",
    "fake_bills = X_train[X_train['class'] == 1]\n",
    "sns.pairplot(fake_bills, hue='color', diag_kind='hist', palette=fake_palette)\n",
    "plt.suptitle('Pairwise Relationships for Fake Bills', y=1.02)\n",
    "plt.savefig('fake_bills.pdf')\n",
    "plt.close()\n",
    "\n",
    "print(\"Pairwise plots have been saved as 'good_bills.pdf' and 'fake_bills.pdf'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66caf44c-9f83-433f-af7d-9a99451b52d3",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "934b8322-00b2-4419-bd28-c9996e125615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_classifier(f1, f2, f3, f4):\n",
    "    # Rule-based classifier\n",
    "    if (f1 > 0) and (f2 > 2) and (f3 < 3) and (f4 < 0):\n",
    "        x = 0 \n",
    "    else:\n",
    "        x = 1 \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce57cd6-c390-4ed5-a5e7-0753e6655554",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16e3905a-2b4f-4a41-9e63-3b9e91b8827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the simple classifier to the test data\n",
    "X_test['predicted_class'] = X_test.apply(lambda row: simple_classifier(row['variance'], row['skewness'], row['curtosis'], row['entropy']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaafbf1-cb0d-4453-bb1d-c7949fca89a7",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1209b32c-46b1-44ef-ac45-8d5ee93a8340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 278, FP: 230, TN: 151, FN: 27\n",
      "TPR (Recall): 0.91\n",
      "TNR (Specificity): 0.40\n"
     ]
    }
   ],
   "source": [
    "# True labels and predicted labels\n",
    "y_true = X_test['class']\n",
    "y_pred = X_test['predicted_class']\n",
    "\n",
    "# Calculate TP, FP, TN, FN\n",
    "TP = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "FP = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "TN = ((y_pred == 0) & (y_true == 0)).sum()\n",
    "FN = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "\n",
    "# Calculate TPR and TNR\n",
    "TPR = TP / (TP + FN) if (TP + FN) != 0 else 0  # True Positive Rate (Recall)\n",
    "TNR = TN / (TN + FP) if (TN + FP) != 0 else 0  # True Negative Rate (Specificity)\n",
    "\n",
    "# Print the results\n",
    "print(f\"TP: {TP}, FP: {FP}, TN: {TN}, FN: {FN}\")\n",
    "print(f\"TPR (Recall): {TPR:.2f}\")\n",
    "print(f\"TNR (Specificity): {TNR:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252471a-ed8a-445e-a119-a8440b32b37f",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ab20eec-6572-4025-9b67-14c104fa14c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TP  FP  TN  FN  accuracy  TPR  TNR\n",
      "278 230 151  27      0.63 0.91  0.4\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN) if (TP + FP + TN + FN) != 0 else 0\n",
    "\n",
    "# Create a DataFrame to summarize the results\n",
    "results_summary = pd.DataFrame({\n",
    "    'TP': [TP],\n",
    "    'FP': [FP],\n",
    "    'TN': [TN],\n",
    "    'FN': [FN],\n",
    "    'accuracy': [round(accuracy, 2)],\n",
    "    'TPR': [round(TPR, 2)],\n",
    "    'TNR': [round(TNR, 2)]\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "print(results_summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b23868-80db-491b-8167-bdccd72c8505",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8089a-5d90-4829-8b52-88ffc4f0e1a2",
   "metadata": {},
   "source": [
    "The simple classifier achieves an overall accuracy of 0.63, which is better than 50% (random guessing). This indicates that the classifier is able to identify bills with an accuracy above random chance.\n",
    "\n",
    "However, when examining the True Positive Rate (TPR) and True Negative Rate (TNR), we see that the TPR is much higher (0.91) compared to the TNR (0.40). This suggests that the classifier performs better at identifying \"fake\" bills (class 1) than \"real\" bills (class 0). In other words, it has a higher accuracy in detecting fake bills, but it struggles with correctly identifying real bills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c23e9e19-6647-403f-902d-d503081ff616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 3: TP = 300, FP = 0, TN = 386, FN = 0, accuracy = 1.00, TPR = 1.00, TNR = 1.00\n",
      "k = 5: TP = 300, FP = 0, TN = 386, FN = 0, accuracy = 1.00, TPR = 1.00, TNR = 1.00\n",
      "k = 7: TP = 300, FP = 0, TN = 386, FN = 0, accuracy = 1.00, TPR = 1.00, TNR = 1.00\n",
      "k = 9: TP = 300, FP = 3, TN = 383, FN = 0, accuracy = 1.00, TPR = 1.00, TNR = 0.99\n",
      "k = 11: TP = 300, FP = 6, TN = 380, FN = 0, accuracy = 0.99, TPR = 1.00, TNR = 0.98\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAAHFCAYAAADmGm0KAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABV00lEQVR4nO3deXhM9/4H8PfMZJLJZkkiG5HNlghRlEYpWmLfWltbRdFSaa21BC3JVTtXSxtUxXaVa1+uLUUtpZYgZBFLECKLBEmIJJPJ+f2RZn5NM8jETM7M5P16Hs/TnPnOyfubUd7mc2ZGIgiCACIiIiIqQSp2ACIiIiJDxJJEREREpAFLEhEREZEGLElEREREGrAkEREREWnAkkRERESkAUsSERERkQYsSUREREQasCQRERERacCSRETl8sMPP0AikcDPz0/sKPQKd+7cgUQiwaJFi8SOQmRUWJKIqFzWrFkDAIiJicHZs2dFTkNEpHssSUSktQsXLiAqKgrdunUDAPzyyy8iJ3qxnJwcsSMQkZFiSSIirRWXonnz5qFVq1bYvHmzxjKSlJSEzz//HG5ubjA3N4erqyv69u2L1NRU9ZonT55g4sSJ8PLygoWFBRwdHdG1a1dcu3YNAPD7779DIpHg999/L3Hu4hHS2rVr1ceGDh0KGxsbXL16FYGBgbC1tcV7770HAIiIiECvXr1Qq1YtKBQK1KlTByNHjkR6enqp3NeuXcOHH34IJycnWFhYoHbt2hg8eDDy8vJw584dmJmZYe7cuaXud+LECUgkEmzdulXjz+3hw4cwNzfHN998o/F7SiQS/PDDDwCKyt3XX38NT09PKBQK2NnZoXnz5vj11181nltbSqUSQ4YMgY2NDfbt26eTcxKZGjOxAxCRcXn+/Dl+/fVXvPnmm/Dz88OwYcMwYsQIbN26FUOGDFGvS0pKwptvvgmlUolp06ahcePGyMjIwKFDh/D48WM4OTkhOzsbrVu3xp07dzBlyhS0bNkST58+xYkTJ5CcnIwGDRponS8/Px89e/bEyJEjMXXqVBQUFAAAbt26hYCAAIwYMQJVq1bFnTt3sGTJErRu3RpXr16FXC4HAERFRaF169ZwcHBAaGgo6tati+TkZOzZswf5+fnw8PBAz549sWLFCkyePBkymUz9vZcvXw5XV1f06dNHY7YaNWqge/fuWLduHUJCQiCV/v+/U8PDw2Fubo6PP/4YADBhwgRs2LABs2fPxhtvvIFnz54hOjoaGRkZWv9M/unJkyd4//33ERcXh+PHj6NZs2avfU4ikyQQEWlh/fr1AgBhxYoVgiAIQnZ2tmBjYyO0adOmxLphw4YJcrlciI2NfeG5QkNDBQBCRETEC9ccO3ZMACAcO3asxPHbt28LAITw8HD1sSFDhggAhDVr1rx0D4WFhYJSqRTu3r0rABB2796tvu3dd98VqlWrJqSlpb0y086dO9XHkpKSBDMzMyEkJOSl33vPnj0CAOHw4cPqYwUFBYKrq6vwwQcfqI/5+fkJvXv3fum5yqr4Z7Vw4ULh9u3bgq+vr+Dr6yvcuXNHJ+cnMlUctxGRVn755RdYWlpi4MCBAAAbGxv069cPJ0+exI0bN9TrDhw4gPbt28PHx+eF5zpw4ADq1auHDh066DTjBx98UOpYWloaRo0aBTc3N5iZmUEul8Pd3R0AEBcXB6BoxHX8+HH0798fNWrUeOH527VrB39/f/z444/qYytWrIBEIsHnn3/+0mxdunSBs7MzwsPD1ccOHTqEBw8eYNiwYepjLVq0wIEDBzB16lT8/vvveP78edk2/xIXL17EW2+9BScnJ/zxxx/q/RORZixJRFRmN2/exIkTJ9CtWzcIgoAnT57gyZMn6Nu3L4D/f8UbUHT9Ta1atV56vrKs0ZaVlRWqVKlS4lhhYSECAwOxY8cOTJ48GUeOHMG5c+fw559/AoC6gDx+/BgqlapMmcaMGYMjR44gPj4eSqUSP//8M/r27QtnZ+eX3s/MzAyffPIJdu7ciSdPngAA1q5dCxcXF3Tq1Em97ocffsCUKVOwa9cutG/fHnZ2dujdu3eJIqqtiIgIpKamYsSIEahWrVq5z0NUWbAkEVGZrVmzBoIgYNu2bahevbr6V/Gr3NatWweVSgWg6Pqb+/fvv/R8ZVmjUCgAAHl5eSWOa7rgGgAkEkmpY9HR0YiKisLChQvx1VdfoV27dnjzzTdhb29fYp2dnR1kMtkrMwHARx99BHt7e/z444/YunUrUlJSEBQU9Mr7AcCnn36K3NxcbN68GY8fP8aePXswePDgEtc3WVtbIyQkBNeuXUNKSgrCwsLw559/okePHmX6HppMmjQJn3/+OQYPHoz169eX+zxElQVLEhGViUqlwrp16+Dt7Y1jx46V+jVx4kQkJyfjwIEDAIrGSseOHUN8fPwLz9mlSxdcv34dR48efeEaDw8PAMCVK1dKHN+zZ0+ZsxcXJwsLixLHV65cWeJrS0tLtG3bFlu3bn1hCSumUCjw+eefY926dViyZAmaNGmCt99+u0x5fHx80LJlS4SHh2PTpk3Iy8vDp59++sL1Tk5OGDp0KD788EPEx8eX+20NpFIpVq5cibFjx2Lo0KEICwsr13mIKg2xL4oiIuOwd+9eAYAwf/58jbc/fPhQsLCwUF9sfP/+fcHFxUVwdHQUli5dKhw5ckTYvn278NlnnwlxcXGCIAhCVlaW0LBhQ8HGxkaYPXu2cPjwYWH37t3ChAkThKNHj6rP3aFDB6F69erCzz//LBw+fFiYMmWKULduXY0XbltbW5fKlp+fL3h7ewvu7u7Cpk2bhIMHDwpBQUFCvXr1BADCzJkz1WsvX74s2NjYCF5eXsKqVauEo0ePCr/++qvw4YcfCllZWSXOe//+fcHMzEwAIKxevVqrn+fKlSsFAEKtWrWEVq1albq9RYsWQmhoqLBr1y7h+PHjwooVKwR7e3shICBAvWbdunWCTCYT1q1b99Lv9fcLt4vNnDlTACAsWLBAq9xElQlLEhGVSe/evQVzc/OXvupr4MCBgpmZmZCSkiIIgiDcu3dPGDZsmODs7CzI5XLB1dVV6N+/v5Camqq+z+PHj4WxY8cKtWvXFuRyueDo6Ch069ZNuHbtmnpNcnKy0LdvX8HOzk6oWrWqMGjQIOHChQtlLkmCIAixsbFCx44dBVtbW6F69epCv379hMTExFIlqXhtv379BHt7e8Hc3FyoXbu2MHToUCE3N7fUedu1ayfY2dkJOTk5ZfkxqmVmZgqWlpYCAOHnn38udfvUqVOF5s2bC9WrVxcsLCwELy8vYfz48UJ6erp6TXh4eKmfgSaaSpIgCMLChQsFAMK3336rVXaiykIiCIIgylNYRERGLi0tDe7u7vjqq6+wYMECseMQkY7xzSSJiLR0//59JCQkYOHChZBKpRg7dqzYkYhID3jhNhGRllavXo127dohJiYG//nPf1CzZk2xIxGRHnDcRkRERKQBn0kiIiIi0oAliYiIiEgDliQiIiIiDfjqtnIqLCzEgwcPYGtrq/FjEIiIiMjwCIKA7OxsuLq6Qip9+XNFLEnl9ODBA7i5uYkdg4iIiMrh3r17r/wwa5akcrK1tQVQ9EP+5yeOvy6lUonDhw8jMDAQcrlcp+c2BNyf8TP1PZr6/gDT3yP3Z/z0tcesrCy4ubmp/x5/GZakcioesVWpUkUvJcnKygpVqlQxyd/83J/xM/U9mvr+ANPfI/dn/PS9x7JcKsMLt4mIiIg0YEkiIiIi0oAliYiIiEgDliQiIiIiDViSiIiIiDRgSSIiIiLSgCWJiIiISAOWJCIiIiINWJKIiIiINGBJMjCqQgFnbz9CZLoEZ28/gqpQEDsSaYGPn/HjY0hExUQtSSdOnECPHj3g6uoKiUSCXbt2vfI+x48fR7NmzaBQKODl5YUVK1aUWrN9+3b4+vrCwsICvr6+2LlzZ6k1P/30Ezw9PaFQKNCsWTOcPHlSF1t6LQejk9F6/lEMWnMB62/IMGjNBbSefxQHo5PFjkZlwMfP+PExJKK/E7UkPXv2DP7+/li+fHmZ1t++fRtdu3ZFmzZtcOnSJUybNg1jxozB9u3b1WvOnDmDAQMG4JNPPkFUVBQ++eQT9O/fH2fPnlWv2bJlC8aNG4fp06fj0qVLaNOmDbp06YLExESd77GsDkYn44uNF5GcmVvieEpmLr7YeJF/SBs4Pn7Gj48hEf2TqB9w26VLF3Tp0qXM61esWIHatWtj6dKlAAAfHx9cuHABixYtwgcffAAAWLp0KTp27Ijg4GAAQHBwMI4fP46lS5fi119/BQAsWbIEw4cPx4gRI9T3OXToEMLCwjB37lwd7rBsVIUCQvbGQtOT+sXHvt0dAx+XKpBJX/2BfIauoKAAj/KApCfPYWamFDvOa1MVCvhmd0ylefyAyvcYSgCE7I1FR19nk3kMiejVRC1J2jpz5gwCAwNLHOvUqRN++eUXKJVKyOVynDlzBuPHjy+1prhY5efnIzIyElOnTi2xJjAwEKdPn37h987Ly0NeXp7666ysLABFn1KsVL7eXxJnbz8q9a/Xf0rLzkPbhb+/1vcxLGYIuSj+iLOimN7jB1Smx1AAkJyZizM309DS007sODpR/OfW6/75Zai4P+Onrz1qcz6jKkkpKSlwcnIqcczJyQkFBQVIT0+Hi4vLC9ekpKQAANLT06FSqV66RpO5c+ciJCSk1PHDhw/DysqqvFsCAESmSwDIXrlOBgH8R6zhKRQAFV79wPDxM1xlfQwPnzyLjDjTupA7IiJC7Ah6xf0ZP13vMScnp8xrjaokAYBEUvIPMkEQSh3XtOafx8qy5u+Cg4MxYcIE9ddZWVlwc3NDYGAgqlSpot0m/sH+9iOsv3HhlevWDXvTJP4Vq1QqERERgY4dO0Iul4sd57Wdvf0Ig9ZUnscPqLyPYWCblnwMjQT3Z/z0tcfiSVBZGFVJcnZ2LvVsT1paGszMzGBvb//SNcXPHDk4OEAmk710jSYWFhawsLAodVwul7/2gxdQxxEuVRVIyczVeE2EBIBzVQUC6jia1PUQuvjZGYLK+vgBlecxBAAXPoZGifszfrreozbnMqr3SQoICCj1tNvhw4fRvHlz9aZftKZVq1YAAHNzczRr1qzUmoiICPWaiiaTSjCzhy8AlHrCv/jrmT18Te4PZ1PBx8/4vewxLDajmw8fQ6JKRtSS9PTpU1y+fBmXL18GUPQS/8uXL6tfih8cHIzBgwer148aNQp3797FhAkTEBcXhzVr1uCXX37B119/rV4zduxYHD58GPPnz8e1a9cwf/58/Pbbbxg3bpx6zYQJE7B69WqsWbMGcXFxGD9+PBITEzFq1KgK2bcmnf1cEDaoKZyrKkocd66qQNigpujs5yJSMioLPn7G70WPYXEtupNR9usYiMg0iDpuu3DhAtq3b6/+uvianyFDhmDt2rVITk4u8d5Fnp6e2L9/P8aPH48ff/wRrq6u+OGHH9Qv/weAVq1aYfPmzZgxYwa++eYbeHt7Y8uWLWjZsqV6zYABA5CRkYHQ0FAkJyfDz88P+/fvh7u7ewXs+sU6+7mgo68zztxMw+GTZxHYpqVJPr1vqvj4GT9Nj2FKVj6+3nYFS3+7jvd8HNHA+fWuQSQi4yFqSWrXrp36wmtN1q5dW+pY27ZtcfHixZeet2/fvujbt+9L14wePRqjR48uU86KJJNK0NLTDhlxAlp62vEvWCPDx8/4/fMxNDMzw8GYVPwWl4qJ/43CrqC3IZcZ1ZUKRFRO/D+diOglJBIJ5rzvh2pWcsQ8yMJPx26JHYmIKghLEhHRKzjaKhDSsyEAYNnRG4h5kClyIiKqCCxJRERl0NPfFZ0bOqOgUMDXW68gv6BQ7EhEpGcsSUREZSCRSPCv3n6obiVHXHIWlh+7KXYkItIzliQiojKqYWuBf/X2AwD8eOwmopM4diMyZSxJRERa6N7YFd0auUBVKGDif6OQV6ASOxIR6QlLEhGRlkJ7NYS9tTniU7Ox7AjHbkSmiiWJiEhL9jYWmP3X2C3s+C1E3XsibiAi0guWJCKicujSyAU9/F2hKhTw9dYo5Co5diMyNSxJRETlFNqzIRxsLHAj7SmW/nZD7DhEpGMsSURE5VTd2hxz+hSN3VaduIVLiY9FTkREusSSRET0GgIbOqN3E1cUCuDYjcjEsCQREb2mWT0booatBW49fIYlEdfFjkNEOsKSRET0mqpZmWNun0YAgJ9PJiDy7iORExGRLrAkERHpQAdfJ3zQtBYEAfh66xU8z+fYjcjYsSQREenItz184VTFArfTn2HR4Xix4xDRa2JJIiLSkaqWcsz7oDEAYM0ft3HuNsduRMaMJYmISIfa13dE/+ZFY7dJ26KQk18gdiQiKieWJCIiHZvR3RcuVRW4m5GDBQc5diMyVixJREQ6VkUhx/y/xm5rT9/BnwkZIiciovJgSSIi0oN36tXAhy3cABSN3Z7lcexGZGxYkoiI9GRaVx/UrGaJe4+eY96Ba2LHISItsSQREemJ7d/Gbhv+vIvTN9NFTkRE2mBJIiLSo9Z1HTDordoAgEnbruApx25ERoMliYhIz4K7+KBWdUskPXmOOfvjxI5DRGXEkkREpGfWFmZY2NcfALDpbCJOXH8ociIiKguWJCKiChDgbY8hAe4AgKnbryArVylyIiJ6FZYkIqIKMqVLA9S2s8KDzFx8t49jNyJDx5JERFRBrMzNsKifPyQSYMuFe/g9Pk3sSET0EixJREQVqIWnHYa28gAATN1+FZnPOXYjMlQsSUREFWxypwbwsLdCSlYu/rUvVuw4RPQCLElERBXM0lymHrtti7yPo9dSxY5ERBqwJBERiaC5hx1GtPYE8NfYLYdjNyJDw5JERCSSiYH14VXDGmnZeQjZGyN2HCL6B5YkIiKRKORFYzepBNhxKQmHY1LEjkREf8OSREQkoqa1q+Ozd7wAANN2RuPxs3yRExFRMZYkIiKRje9QD3UcbZD+NA8z93DsRmQoWJKIiESmkMuwuJ8/ZFIJ9kQ9wMHoZLEjERFYkoiIDIK/WzWM/GvsNn1nNDKe5omciIhYkoiIDMTYDnVRz8kGGc/y8S3HbkSiY0kiIjIQFmYyLO7XBDKpBP+7koz/XeHYjUhMLElERAakUa2qCGrnDQD4Znc00jl2IxINSxIRkYH58t26aOBsi0fP8vHNrmgIgiB2JKJKiSWJiMjAmJtJsbi/P8ykEhyITsFejt2IRMGSRERkgBq6VsWX79YBAHy7Oxpp2bkiJyKqfFiSiIgMVFD7OvB1qYInOUpM38mxG1FFY0kiIjJQclnR2E0ukyAiNhW7Lz8QOxJRpcKSRERkwHxcqmDMu3UBADP3xCA1i2M3oorCkkREZOBGtfNGo5pVkflciWk7rnLsRlRBWJKIiAycXCbFon7+MJdJceRaGrZfTBI7ElGlwJJERGQE6jvbYlzHorFbyN4YpGRy7EakbyxJRERG4vM2XvB3q4bs3AJM3XGFYzciPWNJIiIyEmYyKRb1bQxzMyl+j3+IrRfuix2JyKSxJBERGZG6TraY2LEeAOBf+2Lx4MlzkRMRmS6WJCIiIzOijRfeqF0N2XkFmLKdYzcifWFJIiIyMjKpBIv6+cPCTIqTN9Kx+fw9sSMRmSSWJCIiI+RdwwaTOtUHAMzeF4v7j3NETkRkeliSiIiM1Kdve6K5e3U8y1dh8rYrKCzk2I1Il1iSiIiMlEwqwcJ+/lDIpTh9KwP/OZcodiQik8KSRERkxDwdrDGlcwMAwNz9cbj3iGM3Il1hSSIiMnJDAjzQwtMOOfkqTNoWxbEbkY6wJBERGTmpVIKFfRvDUi7DnwmPsOHPu2JHIjIJLElERCbA3d4awV2Lxm7zDlzD3YxnIiciMn4sSUREJmJQS3cEeNnjuVKFSVv5ajei18WSRERkIqRSCRb0bQxrcxnO3XmEtafviB2JyKiJXpJ++ukneHp6QqFQoFmzZjh58uRL1//444/w8fGBpaUl6tevj/Xr15e4XalUIjQ0FN7e3lAoFPD398fBgwdLrCkoKMCMGTPg6ekJS0tLeHl5ITQ0FIWFhTrfHxFRRXKzs0JwVx8AwIJD15Dw8KnIiYiMl6glacuWLRg3bhymT5+OS5cuoU2bNujSpQsSEzW/10dYWBiCg4Mxa9YsxMTEICQkBEFBQdi7d696zYwZM7By5UosW7YMsbGxGDVqFPr06YNLly6p18yfPx8rVqzA8uXLERcXhwULFmDhwoVYtmyZ3vdMRKRvH7esjdZ1HJCrLMSkbVeg4tiNqFxELUlLlizB8OHDMWLECPj4+GDp0qVwc3NDWFiYxvUbNmzAyJEjMWDAAHh5eWHgwIEYPnw45s+fX2LNtGnT0LVrV3h5eeGLL75Ap06dsHjxYvWaM2fOoFevXujWrRs8PDzQt29fBAYG4sKFC3rfMxGRvkkkEsz7oBFsLMwQefcxwv+4LXYkIqMkWknKz89HZGQkAgMDSxwPDAzE6dOnNd4nLy8PCoWixDFLS0ucO3cOSqXypWtOnTql/rp169Y4cuQIrl+/DgCIiorCqVOn0LVr19feFxGRIahV3QozuhWN3RYeisfNNI7diLRlJtY3Tk9Ph0qlgpOTU4njTk5OSElJ0XifTp06YfXq1ejduzeaNm2KyMhIrFmzBkqlEunp6XBxcUGnTp2wZMkSvPPOO/D29saRI0ewe/duqFQq9XmmTJmCzMxMNGjQADKZDCqVCt999x0+/PDDF+bNy8tDXl6e+uusrCwARddAFRc0XSk+n67Payi4P+Nn6ns0lf2938QZ/7vyACdvZmDify9jy2ctIJNKAJjOHl+E+zN++tqjNueTCIIgyrD6wYMHqFmzJk6fPo2AgAD18e+++w4bNmzAtWvXSt3n+fPnCAoKwoYNGyAIApycnDBo0CAsWLAAqampcHR0xMOHD/HZZ59h7969kEgk8Pb2RocOHRAeHo6cnKK369+8eTMmTZqEhQsXomHDhrh8+TLGjRuHJUuWYMiQIRrzzpo1CyEhIaWOb9q0CVZWVjr6qRAR6dbjPGBelAy5Kgl61lbhvZq8Pokqt5ycHHz00UfIzMxElSpVXrpWtJKUn58PKysrbN26FX369FEfHzt2LC5fvozjx4+/8L5KpRKpqalwcXHBqlWrMGXKFDx58gRS6f9PD3Nzc5GRkQFXV1dMnToV+/btQ0xMDADAzc0NU6dORVBQkHr97NmzsXHjRo3lDND8TJKbmxvS09Nf+UPWllKpREREBDp27Ai5XK7TcxsC7s/4mfoeTW1/2y4mIXhnDOQyCXaPDkBdRxuT2+M/cX/GT197zMrKgoODQ5lKkmjjNnNzczRr1gwRERElSlJERAR69er10vvK5XLUqlULQNGzQt27dy9RkABAoVCgZs2aUCqV2L59O/r376++LScnp9R6mUz20rcAsLCwgIWFhcYs+voNqs9zGwLuz/iZ+h5NZX8DW7jjcGwajsU/RPDOGGz/ohWKt2Uqe3wR7s/46XqP2pxLtJIEABMmTMAnn3yC5s2bIyAgAKtWrUJiYiJGjRoFAAgODkZSUpL6vZCuX7+Oc+fOoWXLlnj8+DGWLFmC6OhorFu3Tn3Os2fPIikpCU2aNEFSUhJmzZqFwsJCTJ48Wb2mR48e+O6771C7dm00bNgQly5dwpIlSzBs2LCK/QEQEVUAiUSCue83RuC/jyPqfiZWnkjA563dxY5FZPBELUkDBgxARkYGQkNDkZycDD8/P+zfvx/u7kX/8yYnJ5d4zySVSoXFixcjPj4ecrkc7du3x+nTp+Hh4aFek5ubixkzZiAhIQE2Njbo2rUrNmzYgGrVqqnXLFu2DN988w1Gjx6NtLQ0uLq6YuTIkfj2228rautERBXKuaoCM3s0xMStUVj623W0rWMndiQigydqSQKA0aNHY/To0RpvW7t2bYmvfXx8SrwppCZt27ZFbGzsS9fY2tpi6dKlWLp0qTZRiYiM2vtNa+JAdDJ+i0vDlJ3RGO4mdiIiwyb6x5IQEVHFkEgkmNOnEapayhHzIBu/PZCIHYnIoLEkERFVIo5VFAjt1RAAcOi+FHHJ2SInIjJcLElERJVMT39XdPRxhEqQYMqOaOQX8MO9iTRhSSIiqmQkEglCe/rA2kxAXEo2fjx2U+xIRAaJJYmIqBJysLFAX8+iZ5B+PHYT0UmZIiciMjwsSUREldQb9gI6N3RCQaGAr7dGcexG9A8sSURElZREAszq4QN7a3NcS8nGsqM3xI5EZFBYkoiIKjF7a3P8q7cfAOCn32/hyv0n4gYiMiAsSURElVzXRi7o3tgFqr/GbnkFKrEjERkEliQiIkJoLz842JjjeupTfP8bx25EAEsSEREBsLM2x+zejQAAK47fwuV7T8QNRGQAWJKIiAgA0NnPGb2auKJQACb+9zJylRy7UeXGkkRERGqzejREDVsL3Hr4DP+OuC52HCJRsSQREZFadWtzzOlTNHZbdTIBkXcfi5yISDwsSUREVEJHXye837QmBAGYtDWKYzeqtFiSiIiolJndG8KpigUS0p9h0aF4seMQiYIliYiISqlqJcfc94vGbr/8cRvn7zwSORFRxWNJIiIijd5t4IR+zWqpx27P8zl2o8qFJYmIiF5oRndfuFRV4E5GDhYcuiZ2HKIKxZJEREQvVNVSjnkfNAYAhP9xB38mZIiciKjisCQREdFLta1XAwPfdAMATN52Bc/yCkRORFQxWJKIiOiVpnfzgWtVBRIf5WD+QY7dqHJgSSIioleyVcixoK8/AGD9mbs4fStd5ERE+seSREREZdK6rgM+blkbQNHY7SnHbmTiWJKIiKjMgrv6oGY1S9x//Bxz98eJHYdIr1iSiIiozGwszLCwb9Gr3f5zNhEnbzwUORGR/rAkERGRVlrVccDgAHcAwJRtV5CdqxQ5EZF+sCQREZHWpnRugNp2VniQmYs5HLuRiWJJIiIirVn/bez267l7OH6dYzcyPSxJRERULi297DG0lQeAorFb5nOO3ci0sCQREVG5Te5cHx72VkjJysXsfbFixyHSKZYkIiIqNytzMyzs5w+JBNgaeR/HrqWJHYlIZ1iSiIjotbzpYYfhb3sCAKbuuILMHI7dyDSwJBER0Wv7ulN9eDlYIzUrDyH7YsSOQ6QTLElERPTaFHIZFvbzh1QC7LiYhIjYVLEjEb02liQiItKJZu7V8VkbLwDAtJ1X8SQnX+RERK+HJYmIiHRmfMd68K5hjYfZeZi1h2M3Mm4sSUREpDMKuQyL+zeBVALsuvwAB6NTxI5EVG4sSUREpFNN3KphZFtvAMCMXVfx6BnHbmScWJKIiEjnxnWoi3pONkh/mo9vd0eLHYeoXFiSiIhI5yzMZFjUzx8yqQT7riRj/9VksSMRaY0liYiI9KJxrWoY3a547BaN9Kd5Iici0g5LEhER6c1X79ZFA2dbPHqWj292RUMQBLEjEZUZSxIREemNuZkUi/r5w0wqwYHoFOy7wrEbGQ+WJCIi0iu/mlUR1L4OAODb3dF4mM2xGxkHliQiItK7oPZ14OtSBY9zlJix6yrHbmQUWJKIiEjv/j52OxSTij1RD8SORPRKLElERFQhfF2rYMx7dQEA3+6OQVpWrsiJiF6OJYmIiCrMF+284VezCjKfKzFtJ8duZNhYkoiIqMLIZVIs7tcEcpkEv8WlYeelJLEjEb0QSxIREVWo+s62GNehHgBg1p4YpGRy7EaGiSWJiIgq3Mh3vOBfqyqycgsQvOMKx25kkFiSiIiowpnJil7tZi6T4lj8Q2yNvC92JKJSWJKIiEgUdZ1sMSGwaOz2r72xSM58LnIiopJYkoiISDSftfHCG7WrITuvAFO289VuZFhYkoiISDQyqQQL+/rD3EyKE9cfYsv5e2JHIlLTuiR5eHggNDQUiYmJ+shDRESVTB1HG0wKrA8AmP2/ONx/nCNyIqIiWpekiRMnYvfu3fDy8kLHjh2xefNm5OXxwwqJiKj8hrX2RDP36niaV4CpHLuRgdC6JH311VeIjIxEZGQkfH19MWbMGLi4uODLL7/ExYsX9ZGRiIhMXNHYrTEUcilO3UzHpnOcVpD4yn1Nkr+/P77//nskJSVh5syZWL16Nd588034+/tjzZo1/FcAERFpxauGDSZ3agAA+O5/cbj3iGM3Ele5S5JSqcR///tf9OzZExMnTkTz5s2xevVq9O/fH9OnT8fHH3+sy5xERFQJDG3lgRYedsjJV2HytisoLOQ/uEk8Ztre4eLFiwgPD8evv/4KmUyGTz75BP/+97/RoEED9ZrAwEC88847Og1KRESmTyqVYEHfxujy/UmcScjAxrN3MTjAQ+xYVElp/UzSm2++iRs3biAsLAz379/HokWLShQkAPD19cXAgQN1FpKIiCoPDwdrTO1S9PfK3P3XkJjBsRuJQ+tnkhISEuDu7v7SNdbW1ggPDy93KCIiqtw+ecsdB6KT8WfCI3y9LQqbP3sLUqlE7FhUyWj9TFJaWhrOnj1b6vjZs2dx4cIFnYQiIqLKTfrXm0xamctw7vYjrDtzR+xIVAlpXZKCgoJw717pd0RNSkpCUFCQTkIRERG52VkhuKsPAGD+wWu4nf5M5ERU2WhdkmJjY9G0adNSx9944w3ExsZqHeCnn36Cp6cnFAoFmjVrhpMnT750/Y8//ggfHx9YWlqifv36WL9+fYnblUolQkND4e3tDYVCAX9/fxw8eLDUeZKSkjBo0CDY29vDysoKTZo0QWRkpNb5iYhIfz5uURtv17FHrrIQk7ZGQcVXu1EF0rokWVhYIDU1tdTx5ORkmJlpd4nTli1bMG7cOEyfPh2XLl1CmzZt0KVLlxd+5ElYWBiCg4Mxa9YsxMTEICQkBEFBQdi7d696zYwZM7By5UosW7YMsbGxGDVqFPr06YNLly6p1zx+/Bhvv/025HI5Dhw4gNjYWCxevBjVqlXTKj8REemXVCrB/A8aw9pchgt3HyP8j9tiR6JKROuS1LFjRwQHByMzM1N97MmTJ5g2bRo6duyo1bmWLFmC4cOHY8SIEfDx8cHSpUvh5uaGsLAwjes3bNiAkSNHYsCAAfDy8sLAgQMxfPhwzJ8/v8SaadOmoWvXrvDy8sIXX3yBTp06YfHixeo18+fPh5ubG8LDw9GiRQt4eHjgvffeg7e3t5Y/DSIi0rda1a0wo7svAGDhoXjcevhU5ERUWWhdkhYvXox79+7B3d0d7du3R/v27eHp6YmUlJQSReRV8vPzERkZicDAwBLHAwMDcfr0aY33ycvLg0KhKHHM0tIS586dg1KpfOmaU6dOqb/es2cPmjdvjn79+sHR0RFvvPEGfv755zJnJyKiijXwTTe0qeuAvIJCfM2xG1UQrd8CoGbNmrhy5Qr+85//ICoqCpaWlvj000/x4YcfQi6Xl/k86enpUKlUcHJyKnHcyckJKSkpGu/TqVMnrF69Gr1790bTpk0RGRmJNWvWQKlUIj09HS4uLujUqROWLFmCd955B97e3jhy5Ah2794NlUqlPk9CQgLCwsIwYcIETJs2DefOncOYMWNgYWGBwYMHa/zeeXl5JT7INysrC0DRNVDFBU1Xis+n6/MaCu7P+Jn6Hk19f4Bx7vG7Xr7ouuw0LiU+wcrjN/BZa88XrjXG/WnD1PcH6G+P2pxPIoj0IWsPHjxAzZo1cfr0aQQEBKiPf/fdd9iwYQOuXbtW6j7Pnz9HUFAQNmzYAEEQ4OTkhEGDBmHBggVITU2Fo6MjHj58iM8++wx79+6FRCKBt7c3OnTogPDwcOTkFL0hmbm5OZo3b17iGasxY8bg/PnzOHPmjMa8s2bNQkhISKnjmzZtgpWV1ev+OIiIqAz+TJPg11symEkETGqsgjP/+CUt5eTk4KOPPkJmZiaqVKny0rVaP5NULDY2FomJicjPzy9xvGfPnmW6v4ODA2QyWalnjdLS0ko9u1TM0tISa9aswcqVK5GamgoXFxesWrUKtra2cHBwAADUqFEDu3btQm5uLjIyMuDq6oqpU6fC0/P//8Xh4uICX1/fEuf28fHB9u3bX5g3ODgYEyZMUH+dlZUFNzc3BAYGvvKHrC2lUomIiAh07NhRq2fnjAX3Z/xMfY+mvj/AePfYRRDwYMMlHL+Rjv9l2GFLnxYwk5W+csRY91dWpr4/QH97LJ4ElUW53nG7T58+uHr1KiQSCYqfiJJIit4J9e9jrZcxNzdHs2bNEBERgT59+qiPR0REoFevXi+9r1wuR61atQAAmzdvRvfu3SGVlvyfRKFQoGbNmlAqldi+fTv69++vvu3tt99GfHx8ifXXr19/6TuJW1hYwMLCQmMWff0G1ee5DQH3Z/xMfY+mvj/AOPc4v68/Ov77OK7cz0L4n/cwul2dF641xv1pw9T3B+h+j9qcS+sLt8eOHQtPT0+kpqbCysoKMTExOHHiBJo3b47ff/9dq3NNmDABq1evxpo1axAXF4fx48cjMTERo0aNAlD07M3frxG6fv06Nm7ciBs3buDcuXMYOHAgoqOjMWfOHPWas2fPYseOHUhISMDJkyfRuXNnFBYWYvLkyeo148ePx59//ok5c+bg5s2b2LRpE1atWsU3wyQiMgLOVRWY2aMhAGBpxA3Ep2SLnIhMldbPJJ05cwZHjx5FjRo1IJVKIZVK0bp1a8ydOxdjxowp8X5ErzJgwABkZGQgNDQUycnJ8PPzw/79+9XP6CQnJ5d4zySVSoXFixcjPj4ecrkc7du3x+nTp+Hh4aFek5ubixkzZiAhIQE2Njbo2rUrNmzYUOI9kN58803s3LkTwcHBCA0NhaenJ5YuXYqPP/5Y2x8HERGJ4IOmNXHgajKOXEvD11ujsGN0K8g1jN2IXofWJUmlUsHGxgZA0XVFDx48QP369eHu7l5qhFUWo0ePxujRozXetnbt2hJf+/j4vLKEtW3btkzv/N29e3d07969zDmJiMhwSCQSzHm/EQL/fQJXkzKx8vgtfPluXbFjkYnRunb7+fnhypUrAICWLVtiwYIF+OOPPxAaGgovLy+dByQiItLEqYoCIT2Lxm7fH7mBuOSyX5BLVBZal6QZM2agsLAQADB79mzcvXsXbdq0wf79+/HDDz/oPCAREdGL9Griio6+TlCqBEz8bxSUqkKxI5EJ0Xrc1qlTJ/V/e3l5ITY2Fo8ePUL16tXVr3AjIiKqCBKJBN/18cP5O48Qm5yFH4/dxLgO9cSORSZCq2eSCgoKYGZmhujo6BLH7ezsWJCIiEgUjrYKhPbyAwAsP3oTV+4/wdnbjxCZLsHZ24/4ESZUblo9k2RmZgZ3d/cyvxcSERFRRejR2AUHribjQHQK3v/pNAoKBQAyrL9xAS5VFZjZwxed/VzEjklGplzXJAUHB+PRo0f6yENERKQ1iUSC9g0cAeCvgvT/UjJz8cXGizgYnSxGNDJiWl+T9MMPP+DmzZtwdXWFu7s7rK2tS9x+8eJFnYUjIiIqC1WhgH9HXNd4mwBAAiBkbyw6+jpDJuXlIVQ2Wpek3r176yEGERFR+Z27/QjJmbkvvF0AkJyZi3O3HyHA277igpFR07okzZw5Ux85iIiIyi0t+8UFqTzriIByXJNERERkaBxtFTpdRwSU45kkqVT60pf785VvRERU0Vp42sGlqgIpmbnQ9IJ/CYo+GLeFp11FRyMjpnVJ2rlzZ4mvlUolLl26hHXr1iEkJERnwYiIiMpKJpVgZg9ffLHxIiRAqaIkAJjZw5cXbZNWtC5JvXr1KnWsb9++aNiwIbZs2YLhw4frJBgREZE2Ovu5IGxQU4TsjS11EbcEgGMVjtpIOzq7Jqlly5b47bffdHU6IiIirXX2c8GpKe9i47DmGFxXhY3DmqNPE1cIAL7eGoVcJS8JobLTSUl6/vw5li1bhlq1aunidEREROUmk0rQ0tMOzRwEtPS0w6yefnC0tUDCw2dYfDhe7HhkRLQet/3zg2wFQUB2djasrKywceNGnYYjIiJ6XVWt5Jj7fiMMX3cBq0/dRqeGzmjuwQu46dW0Lkn//ve/S5QkqVSKGjVqoGXLlqhevbpOwxEREenCez5O6NusFrZF3sekbVewf0wbWJrLxI5FBk7rkjR06FA9xCAiItKvb7r74tSNdNxOf4aFh+LxbQ9fsSORgdP6mqTw8HBs3bq11PGtW7di3bp1OglFRESka1Ut5Zj7QSMAQPjp2zibkCFyIjJ0WpekefPmwcHBodRxR0dHzJkzRyehiIiI9KF9fUcMaO4GQQAmbbuCnPwCsSORAdO6JN29exeenp6ljru7uyMxMVEnoYiIiPRlencfuFZVIPFRDhYc5Kvd6MW0LkmOjo64cuVKqeNRUVGwt+cnKxMRkWGropBjft/GAIC1p+/gzC2O3UgzrUvSwIEDMWbMGBw7dgwqlQoqlQpHjx7F2LFjMXDgQH1kJCIi0qk2dWvgo5a1AQCTtkXhWR7HblSa1iVp9uzZaNmyJd577z1YWlrC0tISgYGBePfdd3lNEhERGY1pXX1Qs5ol7j9+jrkH4sSOQwZI65Jkbm6OLVu2ID4+Hv/5z3+wY8cO3Lp1C2vWrIG5ubk+MhIREemcjYUZFvw1dtv4ZyL+uJkuciIyNFq/T1KxunXrom7durrMQkREVKHeruOAT95yx4Y/72Lytis4OK4NbBVysWORgdD6maS+ffti3rx5pY4vXLgQ/fr100koIiKiijK1SwO42Vki6clzzNl/Tew4ZEC0LknHjx9Ht27dSh3v3LkzTpw4oZNQREREFcXawgwL+/oDAH49l4gT1x+KnIgMhdYl6enTpxqvPZLL5cjKytJJKCIioor0lpc9hrbyAABM2X4FWblKcQORQdC6JPn5+WHLli2ljm/evBm+vvwcHCIiMk6TO9eHu70VkjNz8d0+vtqNynHh9jfffIMPPvgAt27dwrvvvgsAOHLkCDZt2oRt27bpPCAREVFFsDIvGrsNWHUGWy7cQ+dGzmhf31HsWCQirZ9J6tmzJ3bt2oWbN29i9OjRmDhxIpKSknD06FF4eHjoISIREVHFaOFph2FvF3301tTtV5CZw7FbZaZ1SQKAbt264Y8//sCzZ89w8+ZNvP/++xg3bhyaNWum63xEREQV6uvA+vB0sEZqVh5C98WKHYdEVK6SBABHjx7FoEGD4OrqiuXLl6Nr1664cOGCLrMRERFVOEtzGRb1awyJBNh+8T5+i00VOxKJRKtrku7fv4+1a9dizZo1ePbsGfr37w+lUont27fzom0iIjIZzdzt8FkbL6w6kYBpO6+iuUd1VLPip0pUNmV+Jqlr167w9fVFbGwsli1bhgcPHmDZsmX6zEZERCSaCR3rwbuGNdKy8xCyl2O3yqjMJenw4cMYMWIEQkJC0K1bN8hkMn3mIiIiEpVCLsOifv6QSoCdl5JwKCZF7EhUwcpckk6ePIns7Gw0b94cLVu2xPLly/HwId+VlIiITNcbtavj83e8AQDTd0bj8bN8kRNRRSpzSQoICMDPP/+M5ORkjBw5Eps3b0bNmjVRWFiIiIgIZGdn6zMnERGRKMZ1qIu6jjZIf5qHmXtixI5DFUjrV7dZWVlh2LBhOHXqFK5evYqJEydi3rx5cHR0RM+ePfWRkYiISDTFYzeZVII9UQ9w4Gqy2JGogpT7LQAAoH79+liwYAHu37+PX3/9VVeZiIiIDIq/WzV80bZo7DZjVzQynuaJnIgqwmuVpGIymQy9e/fGnj17dHE6IiIig/PVe3VQ38kWGc/y8e1ujt0qA52UJCIiIlNnYSbD4v5FY7f/XU3GvisPxI5EesaSREREVEZ+NasiqH0dAMA3u6LxMJtjN1PGkkRERKSFL9vXgY9LFTzOUWLGrqsQBEHsSKQnLElERERaMDeTYnE/f5hJJTgUk4o9URy7mSqWJCIiIi35ulbBV+/WBQDM3BODtOxckRORPrAkERERlcPo9t5o6FoFT3KUmL4zmmM3E8SSREREVA5ymRSL+/tDLpMgIjYVuy4niR2JdIwliYiIqJwaOFfBuA71AAAzd8cgNYtjN1PCkkRERPQaRr7jhca1qiIrtwDBO/hqN1PCkkRERPQazGRSLOrnD3OZFEevpWH7RY7dTAVLEhER0Wuq52SL8R2Lxm4he2OQnPlc5ESkCyxJREREOvBZG080cauG7NwCTN3OsZspYEkiIiLSAfXYzUyK49cf4r8X7okdiV4TSxIREZGO1HG0wdeBRWO3f+2LQ9ITjt2MGUsSERGRDg1v7YWmtavhaV4Bpm6/wrGbEWNJIiIi0iGZVIJF/fxhYSbFyRvp+PUcx27GiiWJiIhIx7xq2GBy5wYAgO/+F4t7j3JETkTlwZJERESkB5+28sCbHtXxLF+FKduvoLCQYzdjw5JERESkB1KpBAv7+kMhl+L0rQz851yi2JFISyxJREREeuLhYI2pf43d5u6PQ2IGx27GhCWJiIhIjwYHeKClpx1y8lWYtC2KYzcjwpJERESkR8VjNytzGc7efoT1Z+6IHYnKiCWJiIhIz2rbWyG4S9HYbf7BeNxJfyZyIioLliQiIqIK8HFLd7TytsdzJcduxkL0kvTTTz/B09MTCoUCzZo1w8mTJ1+6/scff4SPjw8sLS1Rv359rF+/vsTtSqUSoaGh8Pb2hkKhgL+/Pw4ePPjC882dOxcSiQTjxo3TxXaIiIg0kkolmP9BY1iby3D+zmOEn74jdiR6BVFL0pYtWzBu3DhMnz4dly5dQps2bdClSxckJmp+mWRYWBiCg4Mxa9YsxMTEICQkBEFBQdi7d696zYwZM7By5UosW7YMsbGxGDVqFPr06YNLly6VOt/58+exatUqNG7cWG97JCIiKuZmZ4Xp3XwBAAsOXkPCw6ciJ6KXEbUkLVmyBMOHD8eIESPg4+ODpUuXws3NDWFhYRrXb9iwASNHjsSAAQPg5eWFgQMHYvjw4Zg/f36JNdOmTUPXrl3h5eWFL774Ap06dcLixYtLnOvp06f4+OOP8fPPP6N69ep63ScREVGxD1u4oU1dB+QVFOLrrVFQcexmsMzE+sb5+fmIjIzE1KlTSxwPDAzE6dOnNd4nLy8PCoWixDFLS0ucO3cOSqUScrn8hWtOnTpV4lhQUBC6deuGDh06YPbs2a/Mm5eXh7y8PPXXWVlZAIrGe0ql8pX310bx+XR9XkPB/Rk/U9+jqe8PMP09Gvr+Zvf0Qdflp3Ex8Ql+PnETw9/20Or+hr4/XdDXHrU5n2glKT09HSqVCk5OTiWOOzk5ISUlReN9OnXqhNWrV6N3795o2rQpIiMjsWbNGiiVSqSnp8PFxQWdOnXCkiVL8M4778Db2xtHjhzB7t27oVKp1OfZvHkzLl68iPPnz5c579y5cxESElLq+OHDh2FlZVXm82gjIiJCL+c1FNyf8TP1PZr6/gDT36Mh769HTQk2J8iw6FA8pCmxcLLU/hyGvD9d0fUec3LK/oaeopWkYhKJpMTXgiCUOlbsm2++QUpKCt566y0IggAnJycMHToUCxYsgEwmAwB8//33+Oyzz9CgQQNIJBJ4e3vj008/RXh4OADg3r17GDt2LA4fPlzqGaeXCQ4OxoQJE9RfZ2Vlwc3NDYGBgahSpYq2234ppVKJiIgIdOzYEXK5XKfnNgTcn/Ez9T2a+v4A09+jMeyviyDgwYaLOHEjA/vS7bBlRAuYycp2FYwx7O916WuPxZOgshCtJDk4OEAmk5V61igtLa3Us0vFLC0tsWbNGqxcuRKpqalwcXHBqlWrYGtrCwcHBwBAjRo1sGvXLuTm5iIjIwOurq6YOnUqPD09AQCRkZFIS0tDs2bN1OdVqVQ4ceIEli9fjry8PHXh+jsLCwtYWFiUOi6Xy/X2G1Sf5zYE3J/xM/U9mvr+ANPfo6Hvb35ffwT++wSu3M/C2j/v44t23lrd39D3pwu63qM25xLtwm1zc3M0a9as1NNoERERaNWq1UvvK5fLUatWLchkMmzevBndu3eHVFpyKwqFAjVr1kRBQQG2b9+OXr16AQDee+89XL16FZcvX1b/at68OT7++GNcvnxZY0EiIiLSB5eqlvi2e9Gr3f4dcR03UrNFTkR/J+q4bcKECfjkk0/QvHlzBAQEYNWqVUhMTMSoUaMAFI24kpKS1O+FdP36dZw7dw4tW7bE48ePsWTJEkRHR2PdunXqc549exZJSUlo0qQJkpKSMGvWLBQWFmLy5MkAAFtbW/j5+ZXIYW1tDXt7+1LHiYiI9K1vs1o4EJ2Co9fSMHFrFHZ80arMYzfSL1FL0oABA5CRkYHQ0FAkJyfDz88P+/fvh7u7OwAgOTm5xHsmqVQqLF68GPHx8ZDL5Wjfvj1Onz4NDw8P9Zrc3FzMmDEDCQkJsLGxQdeuXbFhwwZUq1atgndHRET0ahKJBHPfb4SOS47jyv1MrDyRgKD2dcSORTCAC7dHjx6N0aNHa7xt7dq1Jb728fHR+KaQf9e2bVvExsZqleH333/Xaj0REZEuOVVRIKRXQ4zfEoWlv13Hez6OaOCs2xcFkfb4fB4REZEB6N2kJjr4OEGpEjDxv1FQqgrFjlTpsSQREREZAIlEgjnv+6GalRwxD7IQ9vstsSNVeixJREREBsLRVoGQng0BAD8cuYGYB5kiJ6rcWJKIiIgMSE9/V3Ru6IyCQgFfb72C/AKO3cTCkkRERGRAJBIJZvfxg521OeKSs7D82E2xI1VaLElEREQGxsHGAqG9isZuPx27iegkjt3EwJJERERkgLo3dkW3Ri5/jd2ikFegevWdSKdYkoiIiAxUaK+GsLc2x7WUbCw7wrFbRWNJIiIiMlD2NhaY3bvoI7PCjt/ClftPxA1UybAkERERGbAujVzQw98VqsKiN5nk2K3isCQREREZuNCeDeFgY4EbaU+x9LcbYsepNFiSiIiIDFx1a3PM6VM0dlt5/BYu33sibqBKgiWJiIjICAQ2dEafN2qiUACm7IhBPqdueseSREREZCRm9vCFo60FEtKf4cA9/hWub/wJExERGYlqVuaY+34jAMCxZAkuJj4RN5CJY0kiIiIyIu/5OKHPG64QIMGUHdF4zrmb3rAkERERGZkZXeqjqlzAnYwcLDocL3Yck8WSREREZGSqWMox0LsQALDmj9s4d/uRyIlME0sSERGREfKtLqBv05oQBGDytijk5BeIHcnksCQREREZqWld6sGlqgJ3MnKw4CDHbrrGkkRERGSkbBVyzP+gMQBg7ek7+DMhQ+REpoUliYiIyIi9U68GPmxRGwAwaVsUnuVx7KYrLElERERGbno3H9SsZol7j55j/sFrYscxGSxJRERERs7GwgwL+haN3dafuYvTN9NFTmQaWJKIiIhMwNt1HDDoreKx2xU85djttbEkERERmYjgLj6oVd0SSU+eY87+OLHjGD2WJCIiIhNhbWGGhX39AQCbzibi5I2HIicybixJREREJiTA2x5DAtwBAFO2XUF2rlLkRMaLJYmIiMjETOnSALXtrPAgMxff/Y9jt/JiSSIiIjIxVuZmWNTPHxIJsPn8PfwenyZ2JKPEkkRERGSCWnja4dNWngCAqduvIvM5x27aYkkiIiIyUZM61YengzVSsnIxe1+s2HGMDksSERGRibI0l2Fh38aQSICtkfdx9Fqq2JGMCksSERGRCWvuYYcRrf82dsvh2K2sWJKIiIhM3MTA+vCqYY207DyE7I0RO47RYEkiIiIycQq5DIv6+UMqAXZcSkJELMduZcGSREREVAk0rV0dn73jBQCYtvMqHj/LFzmR4WNJIiIiqiTGd6iHOo42eJidh1kcu70SSxIREVEloZDLsLifP2RSCXZffoCD0cliRzJoLElERESViL9bNYxqWzR2m7ErGo84dnshliQiIqJKZsx7dVHfyRbpT/Px7e5oseMYLJYkIiKiSsbCrOjVbjKpBPuuJON/Vzh204QliYiIqBJqVKsqgtp5AwC+2R2N9Kd5IicyPCxJREREldSX79ZFA2dbPHqWj292RUMQBLEjGRSWJCIiokrK3EyKxf39YSaV4EB0CvZx7FYCSxIREVEl1tC1Kr58tw6AorFbWnauyIkMB0sSERFRJRfUvg58XargSY4S03dy7FaMJYmIiKiSk8uKxm5ymQQRsanYffmB2JEMAksSERERwcelCsa+VxcAMHNPDNKyOHZjSSIiIiIAwKi23mhUsyoynysxbefVSj92Y0kiIiIiAIDZX2M3c5kUv8WlYcfFJLEjiYoliYiIiNTqOdliXMeisdusvTFIyay8YzeWJCIiIirh8zZe8HerhuzcAkzdcaXSjt1YkoiIiKgEM5kUi/s1hrmZFL/HP8TWyPtiRxIFSxIRERGVUsfRFhM71gMA/GtvLB48eS5yoorHkkREREQajWjjhTdqV0N2XgGmbK98YzeWJCIiItJIJpVgUT9/WJhJcfJGOjafvyd2pArFkkREREQv5F3DBpM61QcAzN4Xi/uPc0ROVHFYkoiIiOilPn3bE296VMezfFWlGruxJBEREdFLyaQSLOzrD4Vcij9uZuA/ZxPFjlQhWJKIiIjolTwcrDGlcwMAwJz9cbj3yPTHbixJREREVCZDAjzQwtMOOfkqTNoWhcJC0x67sSQRERFRmUilEizq6w8rcxn+THiEjWfvih1Jr1iSiIiIqMxq21thapeisdvc/ddwN+OZyIn0hyWJiIiItDKopTsCvOzxXKnCpK1XTHbsxpJEREREWpFKJVjQtzGszWU4d+cR1p6+I3YkvRC9JP3000/w9PSEQqFAs2bNcPLkyZeu//HHH+Hj4wNLS0vUr18f69evL3G7UqlEaGgovL29oVAo4O/vj4MHD5ZYM3fuXLz55puwtbWFo6Mjevfujfj4eJ3vjYiIyFS52VlhWjcfAMCCQ9dwO930xm6ilqQtW7Zg3LhxmD59Oi5duoQ2bdqgS5cuSEzU/P4LYWFhCA4OxqxZsxATE4OQkBAEBQVh79696jUzZszAypUrsWzZMsTGxmLUqFHo06cPLl26pF5z/PhxBAUF4c8//0RERAQKCgoQGBiIZ89M7wEmIiLSl49a1EbrOg7IVRZi0tYoqExs7CZqSVqyZAmGDx+OESNGwMfHB0uXLoWbmxvCwsI0rt+wYQNGjhyJAQMGwMvLCwMHDsTw4cMxf/78EmumTZuGrl27wsvLC1988QU6deqExYsXq9ccPHgQQ4cORcOGDeHv74/w8HAkJiYiMjJS73smIiIyFRKJBPP7NoaNhRku3H2M8D9uix1Jp8zE+sb5+fmIjIzE1KlTSxwPDAzE6dOnNd4nLy8PCoWixDFLS0ucO3cOSqUScrn8hWtOnTr1wiyZmZkAADs7uxeuycvLQ15envrrrKwsAEXjPaVS+cL7lUfx+XR9XkPB/Rk/U9+jqe8PMP09cn8Vx9HaDMGd62H67lgsPBSP1t528K5h/drn1dcetTmfRBDpA1gePHiAmjVr4o8//kCrVq3Ux+fMmYN169ZpvEZo2rRpCA8Px759+9C0aVNERkaiW7duSEtLw4MHD+Di4oKPPvoIUVFR2LVrF7y9vXHkyBH06tULKpWqRMkpJggCevXqhcePH7/0eqhZs2YhJCSk1PFNmzbBysqqnD8FIiIi4ycIwIo4Ka5lSuFhI2CsnwpSidipNMvJycFHH32EzMxMVKlS5aVrRXsmqZhEUvKnKAhCqWPFvvnmG6SkpOCtt96CIAhwcnLC0KFDsWDBAshkMgDA999/j88++wwNGjSARCKBt7c3Pv30U4SHh2s855dffokrV6689JkmAAgODsaECRPUX2dlZcHNzQ2BgYGv/CFrS6lUIiIiAh07doRcLtfpuQ0B92f8TH2Ppr4/wPT3yP1VvKatc9Ft+WnceVqA5Ko++Ky152udT197LJ4ElYVoJcnBwQEymQwpKSkljqelpcHJyUnjfSwtLbFmzRqsXLkSqampcHFxwapVq2BrawsHBwcAQI0aNbBr1y7k5uYiIyMDrq6umDp1Kjw9Sz9YX331Ffbs2YMTJ06gVq1aL81rYWEBCwuLUsflcrnefoPq89yGgPszfqa+R1PfH2D6e+T+Kk5tBzm+6e6LyduuYOmRW+jo64K6TravfV5d71Gbc4l24ba5uTmaNWuGiIiIEscjIiJKjN80kcvlqFWrFmQyGTZv3ozu3btDKi25FYVCgZo1a6KgoADbt29Hr1691LcJgoAvv/wSO3bswNGjRzUWKCIiItJOv2a10L5+DeQXFOLrrVEoUBWKHem1iPrqtgkTJmD16tVYs2YN4uLiMH78eCQmJmLUqFEAikZcgwcPVq+/fv06Nm7ciBs3buDcuXMYOHAgoqOjMWfOHPWas2fPYseOHUhISMDJkyfRuXNnFBYWYvLkyeo1QUFB2LhxIzZt2gRbW1ukpKQgJSUFz58/r7jNExERmRiJRIK57zdGFYUZou5nYuWJBLEjvRZRr0kaMGAAMjIyEBoaiuTkZPj5+WH//v1wd3cHACQnJ5d4zySVSoXFixcjPj4ecrkc7du3x+nTp+Hh4aFek5ubixkzZiAhIQE2Njbo2rUrNmzYgGrVqqnXFL/FQLt27UrkCQ8Px9ChQ/W1XSIiIpPnXFWBWT0bYsJ/o/D9bzfQwccJ9Z1ff+wmBtEv3B49ejRGjx6t8ba1a9eW+NrHx6fEm0Jq0rZtW8TGxr50jUgv6CMiIqoU+rxRE/uvpuC3uFR8vTUKO0a3glwm+od8aM34EhMREZFBk0gkmNPHD1Ut5bialIkVv98SO1K5sCQRERGRzjlWUSC0V0MAwA9HbyAuuewvvTcULElERESkFz39XdGpoROUKgET/xsFpZG92o0liYiIiPRCIpFgdu9GqG4lR2xyFn48dlPsSFphSSIiIiK9qWFrgdBefgCA5UdvIjopU+REZceSRERERHrVvbELujZyRkGhgK+3RiG/wDjGbixJREREpFcSiQT/6uUHe2tzXEvJxrKjN8SOVCYsSURERKR39jYWmN27aOz20++3cPW+4Y/dWJKIiIioQnRp5IIe/q5QFQqYuPUy8gpUYkd6KZYkIiIiqjAhPRvCwcYc11Of4vvfDHvsxpJEREREFcbO2hyzezcCAKw4fguX7z0RN9BLsCQRERFRhers54zeTVxRKABfb41CrtIwx24sSURERFThZvVsiBq2FriZ9hT//u262HE0YkkiIiKiClfNyhxz+xSN3X4+kYDIu49FTlQaSxIRERGJooOvE95vWhOFAjDJAMduLElEREQkmpndG8KpigUS0p9h0aF4seOUwJJEREREoqlqJce89xsDAH754zYu3HkkcqL/x5JEREREomrfwBH9m9eC8Ner3Z7mFuDs7UeITJfg7O1HUBUKouQyE+W7EhEREf3NjO6+OHkjHXcyctBizm/IyVcBkGH9jQtwqarAzB6+6OznUqGZ+EwSERERia6KQo6+zWoBwF8F6f+lZObii40XcTA6uUIzsSQRERGR6FSFArZF3td4W/GwLWRvbIWO3liSiIiISHTnbj9CcmbuC28XACRn5uLc7Yq7sJsliYiIiESXlv3iglSedbrAkkRERESic7RV6HSdLrAkERERkehaeNrBpaoCkhfcLgHgUlWBFp52FZaJJYmIiIhEJ5NKMLOHLwCUKkrFX8/s4QuZ9EU1SvdYkoiIiMggdPZzQdigpnCuWnKk5lxVgbBBTSv8fZL4ZpJERERkMDr7uaCjrzPO3EzD4ZNnEdimJQLqOFboM0jFWJKIiIjIoMikErT0tENGnICWnnaiFCSA4zYiIiIijViSiIiIiDRgSSIiIiLSgCWJiIiISAOWJCIiIiINWJKIiIiINGBJIiIiItKAJYmIiIhIA5YkIiIiIg34jtvlJAgCACArK0vn51YqlcjJyUFWVhbkcrnOzy827s/4mfoeTX1/gOnvkfszfvraY/Hf28V/j78MS1I5ZWdnAwDc3NxETkJERETays7ORtWqVV+6RiKUpUpRKYWFhXjw4AFsbW0hkej2M2WysrLg5uaGe/fuoUqVKjo9tyHg/oyfqe/R1PcHmP4euT/jp689CoKA7OxsuLq6Qip9+VVHfCapnKRSKWrVqqXX71GlShWT/c0PcH+mwNT3aOr7A0x/j9yf8dPHHl/1DFIxXrhNREREpAFLEhEREZEGLEkGyMLCAjNnzoSFhYXYUfSC+zN+pr5HU98fYPp75P6MnyHskRduExEREWnAZ5KIiIiINGBJIiIiItKAJYmIiIhIA5YkIiIiIg1YkgxEWFgYGjdurH7TrICAABw4cEDsWHozd+5cSCQSjBs3TuwoOjNr1ixIJJISv5ydncWOpVNJSUkYNGgQ7O3tYWVlhSZNmiAyMlLsWDrj4eFR6jGUSCQICgoSO5pOFBQUYMaMGfD09ISlpSW8vLwQGhqKwsJCsaPpTHZ2NsaNGwd3d3dYWlqiVatWOH/+vNixyu3EiRPo0aMHXF1dIZFIsGvXrhK3C4KAWbNmwdXVFZaWlmjXrh1iYmLECVsOr9rfjh070KlTJzg4OEAikeDy5csVmo8lyUDUqlUL8+bNw4ULF3DhwgW8++676NWrl1H9Zi+r8+fPY9WqVWjcuLHYUXSuYcOGSE5OVv+6evWq2JF05vHjx3j77bchl8tx4MABxMbGYvHixahWrZrY0XTm/PnzJR6/iIgIAEC/fv1ETqYb8+fPx4oVK7B8+XLExcVhwYIFWLhwIZYtWyZ2NJ0ZMWIEIiIisGHDBly9ehWBgYHo0KEDkpKSxI5WLs+ePYO/vz+WL1+u8fYFCxZgyZIlWL58Oc6fPw9nZ2d07NhR/fmihu5V+3v27BnefvttzJs3r4KT/UUgg1W9enVh9erVYsfQqezsbKFu3bpCRESE0LZtW2Hs2LFiR9KZmTNnCv7+/mLH0JspU6YIrVu3FjtGhRo7dqzg7e0tFBYWih1FJ7p16yYMGzasxLH3339fGDRokEiJdCsnJ0eQyWTCvn37Shz39/cXpk+fLlIq3QEg7Ny5U/11YWGh4OzsLMybN099LDc3V6hataqwYsUKERK+nn/u7+9u374tABAuXbpUoZn4TJIBUqlU2Lx5M549e4aAgACx4+hUUFAQunXrhg4dOogdRS9u3LgBV1dXeHp6YuDAgUhISBA7ks7s2bMHzZs3R79+/eDo6Ig33ngDP//8s9ix9CY/Px8bN27EsGHDdP4h1mJp3bo1jhw5guvXrwMAoqKicOrUKXTt2lXkZLpRUFAAlUoFhUJR4rilpSVOnTolUir9uX37NlJSUhAYGKg+ZmFhgbZt2+L06dMiJjMd/IBbA3L16lUEBAQgNzcXNjY22LlzJ3x9fcWOpTObN2/GxYsXjfr6gJdp2bIl1q9fj3r16iE1NRWzZ89Gq1atEBMTA3t7e7HjvbaEhASEhYVhwoQJmDZtGs6dO4cxY8bAwsICgwcPFjuezu3atQtPnjzB0KFDxY6iM1OmTEFmZiYaNGgAmUwGlUqF7777Dh9++KHY0XTC1tYWAQEB+Ne//gUfHx84OTnh119/xdmzZ1G3bl2x4+lcSkoKAMDJyanEcScnJ9y9e1eMSCaHJcmA1K9fH5cvX8aTJ0+wfft2DBkyBMePHzeJonTv3j2MHTsWhw8fLvWvPFPRpUsX9X83atQIAQEB8Pb2xrp16zBhwgQRk+lGYWEhmjdvjjlz5gAA3njjDcTExCAsLMwkS9Ivv/yCLl26wNXVVewoOrNlyxZs3LgRmzZtQsOGDXH58mWMGzcOrq6uGDJkiNjxdGLDhg0YNmwYatasCZlMhqZNm+Kjjz7CxYsXxY6mN/98plMQBJN59lNsHLcZEHNzc9SpUwfNmzfH3Llz4e/vj++//17sWDoRGRmJtLQ0NGvWDGZmZjAzM8Px48fxww8/wMzMDCqVSuyIOmdtbY1GjRrhxo0bYkfRCRcXl1KF3cfHB4mJiSIl0p+7d+/it99+w4gRI8SOolOTJk3C1KlTMXDgQDRq1AiffPIJxo8fj7lz54odTWe8vb1x/PhxPH36FPfu3cO5c+egVCrh6ekpdjSdK371bPEzSsXS0tJKPbtE5cOSZMAEQUBeXp7YMXTivffew9WrV3H58mX1r+bNm+Pjjz/G5cuXIZPJxI6oc3l5eYiLi4OLi4vYUXTi7bffRnx8fIlj169fh7u7u0iJ9Cc8PByOjo7o1q2b2FF0KicnB1JpyT/2ZTKZSb0FQDFra2u4uLjg8ePHOHToEHr16iV2JJ3z9PSEs7Oz+lWYQNG1dMePH0erVq1ETGY6OG4zENOmTUOXLl3g5uaG7OxsbN68Gb///jsOHjwodjSdsLW1hZ+fX4lj1tbWsLe3L3XcWH399dfo0aMHateujbS0NMyePRtZWVkmM8YYP348WrVqhTlz5qB///44d+4cVq1ahVWrVokdTacKCwsRHh6OIUOGwMzMtP6I7NGjB7777jvUrl0bDRs2xKVLl7BkyRIMGzZM7Gg6c+jQIQiCgPr16+PmzZuYNGkS6tevj08//VTsaOXy9OlT3Lx5U/317du3cfnyZdjZ2aF27doYN24c5syZg7p166Ju3bqYM2cOrKys8NFHH4mYuuxetb9Hjx4hMTERDx48AAD1P9ScnZ0r5n3oKvS1dPRCw4YNE9zd3QVzc3OhRo0awnvvvSccPnxY7Fh6ZWpvATBgwADBxcVFkMvlgqurq/D+++8LMTExYsfSqb179wp+fn6ChYWF0KBBA2HVqlViR9K5Q4cOCQCE+Ph4saPoXFZWljB27Fihdu3agkKhELy8vITp06cLeXl5YkfTmS1btgheXl6Cubm54OzsLAQFBQlPnjwRO1a5HTt2TABQ6teQIUMEQSh6G4CZM2cKzs7OgoWFhfDOO+8IV69eFTe0Fl61v/DwcI23z5w5s0LySQRBEPRfxYiIiIiMC69JIiIiItKAJYmIiIhIA5YkIiIiIg1YkoiIiIg0YEkiIiIi0oAliYiIiEgDliQiIiIiDViSiIj+0q5dO4wbN07sGERkIFiSiIiIiDRgSSIiIiLSgCWJiOgFDh48iKpVq2L9+vViRyEiEbAkERFpsHnzZvTv3x/r16/H4MGDxY5DRCJgSSIi+oeffvoJo0aNwu7du9GrVy+x4xCRSMzEDkBEZEi2b9+O1NRUnDp1Ci1atBA7DhGJiM8kERH9TZMmTVCjRg2Eh4dDEASx4xCRiFiSiIj+xtvbG8eOHcPu3bvx1VdfiR2HiETEcRsR0T/Uq1cPx44dQ7t27WBmZoalS5eKHYmIRMCSRESkQf369XH06FG0a9cOMpkMixcvFjsSEVUwicChOxEREVEpvCaJiIiISAOWJCIiIiINWJKIiIiINGBJIiIiItKAJYmIiIhIA5YkIiIiIg1YkoiIiIg0YEkiIiIi0oAliYiIiEgDliQiIiIiDViSiIiIiDRgSSIiIiLS4P8AH+x+jAvSvwEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of k* is 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = df[['variance', 'skewness', 'curtosis', 'entropy']]\n",
    "y = df['class']\n",
    "\n",
    "# List to store accuracies for different k values\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over different k values\n",
    "for k in [3, 5, 7, 9, 11]:\n",
    "    # Split into 50/50 training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Train k-NN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Calculate confusion matrix values\n",
    "    TN, FP, FN, TP = confusion_matrix(y_test, y_pred).ravel()\n",
    "    \n",
    "    # Calculate TPR and TNR\n",
    "    TPR = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "    TNR = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "    \n",
    "    # Print results for current k\n",
    "    print(f\"k = {k}: TP = {TP}, FP = {FP}, TN = {TN}, FN = {FN}, accuracy = {accuracy:.2f}, TPR = {TPR:.2f}, TNR = {TNR:.2f}\")\n",
    "\n",
    "# Plot accuracy vs. k\n",
    "plt.plot([3, 5, 7, 9, 11], accuracies, marker='o')\n",
    "plt.title('Accuracy vs. k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal k (k*)\n",
    "k_star = [3, 5, 7, 9, 11][accuracies.index(max(accuracies))]\n",
    "print(f\"The optimal value of k* is {k_star}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f7f3c89c-a9c5-4f58-b19e-b19eeae8b692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TP  FP  TN  FN  accuracy  TPR  TNR\n",
      "300   0 386   0       1.0  1.0  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Train k-NN classifier with the optimal k* value\n",
    "k_optimal = 3\n",
    "knn = KNeighborsClassifier(n_neighbors=k_optimal)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the class labels for the test set\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "# Calculate TP, FP, TN, FN using confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_knn).ravel()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "# Calculate TPR and TNR\n",
    "tpr_knn = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "tnr_knn = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "# Summarize the findings in a table\n",
    "summary_table = pd.DataFrame({\n",
    "    'TP': [tp],\n",
    "    'FP': [fp],\n",
    "    'TN': [tn],\n",
    "    'FN': [fn],\n",
    "    'accuracy': [round(accuracy_knn, 2)],\n",
    "    'TPR': [round(tpr_knn, 2)],\n",
    "    'TNR': [round(tnr_knn, 2)]\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(summary_table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1815fa2-1e4f-47d0-bdff-902d493887f8",
   "metadata": {},
   "source": [
    "Yes, the k-NN classifier performs significantly better than the simple classifier in all measures:\n",
    "\n",
    "Accuracy: The k-NN classifier achieves an accuracy of 1.00 (100%) for k=3,5,7, whereas the simple classifier's accuracy was only 0.63 (63%).\n",
    "\n",
    "TPR (Recall): 1.00 (100%) for all values of k, since all positive cases are correctly identified.\n",
    "\n",
    "TNR (Specificity): Close to 1.00 (100%) for values of 𝑘=3,5,7, indicating that almost all negative cases are also correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2008369-d83d-42e0-b724-75764f66eb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Classifier Prediction: fake\n"
     ]
    }
   ],
   "source": [
    "simple_class_label = simple_classifier(6, 7, 8, 2)\n",
    "print(f\"Simple Classifier Prediction: {simple_class_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "75f4bdac-cc38-4c42-a099-802641e0dde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Classifier Prediction: good\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Create a DataFrame for the new data point with the correct feature names\n",
    "new_data = pd.DataFrame([[6, 7, 8, 2]], columns=['variance', 'skewness', 'curtosis', 'entropy'])\n",
    "\n",
    "# Predict the class label\n",
    "knn_class_label = knn.predict(new_data)\n",
    "print(f\"k-NN Classifier Prediction: {'good' if knn_class_label[0] == 0 else 'fake'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "88f19128-3d71-4e05-b25f-dbc1ee54f2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with variance dropped: 0.96\n",
      "Accuracy with skewness dropped: 0.98\n",
      "Accuracy with curtosis dropped: 0.98\n",
      "Accuracy with entropy dropped: 0.99\n",
      "Accuracy with all features: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to train and evaluate k-NN accuracy with a given feature set\n",
    "def evaluate_knn_accuracy(features_to_drop, k_optimal):\n",
    "    # Drop the specified features from X_train and X_test\n",
    "    X_train_truncated = X_train.drop(columns=features_to_drop)\n",
    "    X_test_truncated = X_test.drop(columns=features_to_drop)\n",
    "    \n",
    "    # Train the k-NN classifier on the truncated feature set\n",
    "    knn = KNeighborsClassifier(n_neighbors=k_optimal)\n",
    "    knn.fit(X_train_truncated, y_train)\n",
    "    \n",
    "    # Predict the labels for the truncated test set\n",
    "    y_pred_truncated = knn.predict(X_test_truncated)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy_truncated = accuracy_score(y_test, y_pred_truncated)\n",
    "    return round(accuracy_truncated, 2)\n",
    "\n",
    "# List of feature sets to drop one at a time\n",
    "features = ['variance', 'skewness', 'curtosis', 'entropy']\n",
    "accuracies = {}\n",
    "\n",
    "# Evaluate k-NN accuracy by dropping each feature one by one\n",
    "for feature in features:\n",
    "    accuracy = evaluate_knn_accuracy([feature], k_optimal)\n",
    "    accuracies[feature] = accuracy\n",
    "    print(f\"Accuracy with {feature} dropped: {accuracy}\")\n",
    "\n",
    "# Accuracy with all features\n",
    "accuracy_all_features = evaluate_knn_accuracy([], k_optimal)\n",
    "print(f\"Accuracy with all features: {accuracy_all_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "187ea693-58c1-494d-9402-c230654b2284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Did accuracy increase when a feature was removed?: False\n"
     ]
    }
   ],
   "source": [
    "# Analysis\n",
    "accuracy_increase = any(accuracy > accuracy_all_features for accuracy in accuracies.values())\n",
    "most_impact = min(accuracies, key=accuracies.get)\n",
    "least_impact = max(accuracies, key=accuracies.get)\n",
    "\n",
    "print(\"\\nDid accuracy increase when a feature was removed?:\", accuracy_increase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1939a9f9-bdcc-414d-b5fd-def4c49c3f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature contributing the most to loss of accuracy when removed: variance\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature contributing the most to loss of accuracy when removed:\", most_impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eb1cd079-ea9a-45d2-bb82-29d53947d794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature contributing the least to loss of accuracy when removed: entropy\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature contributing the least to loss of accuracy when removed:\", least_impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "732163ee-ae98-4a88-99c5-66f6c426d52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TP  FP  TN  FN  accuracy  TPR  TNR\n",
      "298   4 382   2      0.99 0.99 0.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming the dataset is already loaded into a DataFrame named df with features and target\n",
    "# Separate features and target\n",
    "X = df[['variance', 'skewness', 'curtosis', 'entropy']]\n",
    "y = df['class']\n",
    "\n",
    "# Step 1: Train logistic regression model\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train the logistic regression classifier\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the class labels for the test set\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Step 2: Calculate performance metrics\n",
    "# Confusion matrix components\n",
    "TP = ((y_pred_log_reg == 1) & (y_test == 1)).sum()\n",
    "FP = ((y_pred_log_reg == 1) & (y_test == 0)).sum()\n",
    "TN = ((y_pred_log_reg == 0) & (y_test == 0)).sum()\n",
    "FN = ((y_pred_log_reg == 0) & (y_test == 1)).sum()\n",
    "\n",
    "# Accuracy, TPR (Recall), and TNR (Specificity)\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "TPR_log_reg = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "TNR_log_reg = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "\n",
    "# Step 3: Create the DataFrame to display results\n",
    "results_df = pd.DataFrame({\n",
    "    'TP': [TP],\n",
    "    'FP': [FP],\n",
    "    'TN': [TN],\n",
    "    'FN': [FN],\n",
    "    'accuracy': [round(accuracy_log_reg, 2)],\n",
    "    'TPR': [round(TPR_log_reg, 2)],\n",
    "    'TNR': [round(TNR_log_reg, 2)]\n",
    "})\n",
    "\n",
    "# Display the DataFrame in the desired format\n",
    "print(results_df.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa174e42-71f1-4b98-9cc3-2b0befa01995",
   "metadata": {},
   "source": [
    "The logistic regression classifier is significantly better than the simple classifier in all metrics (TP, FP, TN, FN, accuracy, TPR, and TNR). The logistic regression model achieves a higher accuracy and performs better at distinguishing both true positives and true negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7088c7-79cc-4533-9a91-2d2ecdf0f590",
   "metadata": {},
   "source": [
    "The k-NN classifier with 𝑘=3 performs slightly better than logistic regression. It achieves perfect accuracy (1.00) with no false positives or false negatives, while logistic regression has slightly lower metrics due to a few false positives (FP = 4) and false negatives (FN = 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b2c1370c-5926-4565-8f97-710faffc5665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class label predicted by logistic regression for the bill with features [6, 7, 8, 2] is: 0\n"
     ]
    }
   ],
   "source": [
    "# Predict for a bill containing the last 4 digits of your BUID (6782)\n",
    "bill_features = pd.DataFrame([[6, 7, 8, 2]], columns=['variance', 'skewness', 'curtosis', 'entropy'])  # Use a DataFrame with feature names\n",
    "predicted_label_log_reg = log_reg.predict(bill_features)[0]\n",
    "\n",
    "# Output the result\n",
    "print(f\"The class label predicted by logistic regression for the bill with features [6, 7, 8, 2] is: {predicted_label_log_reg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5820197c-1ddd-45c2-9c8c-4624c4d7c64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with all features: 0.99\n",
      "Accuracy without f1 (variance): 0.81\n",
      "Accuracy without f2 (skewness): 0.91\n",
      "Accuracy without f3 (curtosis): 0.87\n",
      "Accuracy without f4 (entropy): 0.99\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming the dataset is already loaded into a DataFrame named df with features and target\n",
    "# Separate features and target\n",
    "X = df[['variance', 'skewness', 'curtosis', 'entropy']]\n",
    "y = df['class']\n",
    "\n",
    "# Function to compute accuracy after removing one feature\n",
    "def evaluate_feature_removal(feature_to_remove):\n",
    "    if feature_to_remove:  # If a feature is specified to be removed\n",
    "        # Drop the specified feature\n",
    "        X_reduced = X.drop(columns=[feature_to_remove])\n",
    "    else:\n",
    "        # Use all features if no feature is specified to be removed\n",
    "        X_reduced = X\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Train the logistic regression classifier\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the class labels for the test set\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate accuracy for all scenarios\n",
    "accuracy_all_features = evaluate_feature_removal(feature_to_remove=None) # Evaluate with all features\n",
    "accuracy_f1_missing = evaluate_feature_removal('variance')\n",
    "accuracy_f2_missing = evaluate_feature_removal('skewness')\n",
    "accuracy_f3_missing = evaluate_feature_removal('curtosis')\n",
    "accuracy_f4_missing = evaluate_feature_removal('entropy')\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy with all features: {accuracy_all_features:.2f}\")\n",
    "print(f\"Accuracy without f1 (variance): {accuracy_f1_missing:.2f}\")\n",
    "print(f\"Accuracy without f2 (skewness): {accuracy_f2_missing:.2f}\")\n",
    "print(f\"Accuracy without f3 (curtosis): {accuracy_f3_missing:.2f}\")\n",
    "print(f\"Accuracy without f4 (entropy): {accuracy_f4_missing:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1841338-8db6-48bf-a904-fc591bdb6b1f",
   "metadata": {},
   "source": [
    "No, the accuracy did not increase in any of the 4 cases when a feature was removed compared to when all 4 features were used. The highest accuracy achieved without any feature was still 0.99, which matched the accuracy with all features present. In cases where features were removed (especially f1, f2, and f3), the accuracy decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b7d342ce-86e5-4451-bf0f-8c9f255328b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature whose removal caused the most accuracy drop: f1 (variance)\n",
      "Feature whose removal caused the least accuracy drop: f4 (entropy)\n"
     ]
    }
   ],
   "source": [
    "# Determine the feature that contributed the most and least to accuracy loss\n",
    "accuracy_drop = {\n",
    "    'f1 (variance)': accuracy_all_features - accuracy_f1_missing,\n",
    "    'f2 (skewness)': accuracy_all_features - accuracy_f2_missing,\n",
    "    'f3 (curtosis)': accuracy_all_features - accuracy_f3_missing,\n",
    "    'f4 (entropy)': accuracy_all_features - accuracy_f4_missing\n",
    "}\n",
    "\n",
    "most_impactful_feature = max(accuracy_drop, key=accuracy_drop.get)\n",
    "least_impactful_feature = min(accuracy_drop, key=accuracy_drop.get)\n",
    "\n",
    "print(f\"Feature whose removal caused the most accuracy drop: {most_impactful_feature}\")\n",
    "print(f\"Feature whose removal caused the least accuracy drop: {least_impactful_feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02fdead-7ded-46b1-9c34-badc95686166",
   "metadata": {},
   "source": [
    "The relative significance of features obtained from logistic regression aligns with the findings from the k-NN classifier. Both approaches suggest that variance is the most important feature, while entropy is the least important."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
